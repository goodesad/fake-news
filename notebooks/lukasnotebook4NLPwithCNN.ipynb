{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdd2b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e571287",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '../raw_data'\n",
    "cleandf = pd.read_csv(os.path.join(csv_path, 'clean_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bdc31c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>smell hillary fear daniel greenfield shillman ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>watch exact moment paul ryan commit political ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>kerry go paris gesture sympathy u secretary st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>bernie supporter twitter erupt anger dnc try w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>battle new york primary matter primary day new...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6330</th>\n",
       "      <td>6330</td>\n",
       "      <td>state department say can find email clinton sp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331</th>\n",
       "      <td>6331</td>\n",
       "      <td>p pb stand plutocratic pentagon p pb stand plu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6332</th>\n",
       "      <td>6332</td>\n",
       "      <td>anti trump protester tool oligarchy informatio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6333</th>\n",
       "      <td>6333</td>\n",
       "      <td>ethiopia obama seek progress peace security ea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6334</th>\n",
       "      <td>6334</td>\n",
       "      <td>jeb bush suddenly attack trump matter jeb bush...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6335 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text  target\n",
       "0              0  smell hillary fear daniel greenfield shillman ...       0\n",
       "1              1  watch exact moment paul ryan commit political ...       0\n",
       "2              2  kerry go paris gesture sympathy u secretary st...       1\n",
       "3              3  bernie supporter twitter erupt anger dnc try w...       0\n",
       "4              4  battle new york primary matter primary day new...       1\n",
       "...          ...                                                ...     ...\n",
       "6330        6330  state department say can find email clinton sp...       1\n",
       "6331        6331  p pb stand plutocratic pentagon p pb stand plu...       0\n",
       "6332        6332  anti trump protester tool oligarchy informatio...       0\n",
       "6333        6333  ethiopia obama seek progress peace security ea...       1\n",
       "6334        6334  jeb bush suddenly attack trump matter jeb bush...       1\n",
       "\n",
       "[6335 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleandf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dec40ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:10:54.083882: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-08-22 17:10:54.168860: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-22 17:10:54.168879: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Embedding, Flatten, Masking, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "830a5847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 17:10:57.941092: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-22 17:10:57.941139: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-22 17:10:57.941161: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-0UJ0SON8): /proc/driver/nvidia/version does not exist\n",
      "2022-08-22 17:10:57.942075: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "model = Sequential()\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(cleandf['text'], cleandf['target'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75329dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e30a2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_token = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b79eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token = pad_sequences(X_train_token, dtype='int32', padding='post', maxlen=300)\n",
    "X_test_token = pad_sequences(X_test_token, dtype='int32', padding='post', maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8f12ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(\n",
    "    input_dim=(len(tokenizer.word_index))+1, # +1 for the 0 padding\n",
    "    input_length=300, # Max_sentence_length (optional, for model summary)\n",
    "    output_dim=100,\n",
    "    mask_zero=True, # Built-in masking layer :)\n",
    "))\n",
    "#model.add(GRU(20, return_sequences=True))\n",
    "model.add(Conv1D(30, kernel_size=5, activation='tanh'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(15, activation='relu'))\n",
    "#model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='rmsprop',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d865e1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 300, 100)          4960900   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 296, 30)           15030     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8880)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 15)                133215    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,109,161\n",
      "Trainable params: 5,109,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dfa67fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "127/127 [==============================] - 3s 17ms/step - loss: 0.4185 - accuracy: 0.7955 - val_loss: 0.2522 - val_accuracy: 0.8955\n",
      "Epoch 2/50\n",
      "127/127 [==============================] - 2s 16ms/step - loss: 0.1253 - accuracy: 0.9521 - val_loss: 0.3311 - val_accuracy: 0.8876\n",
      "Epoch 3/50\n",
      "127/127 [==============================] - 2s 16ms/step - loss: 0.0292 - accuracy: 0.9906 - val_loss: 0.3220 - val_accuracy: 0.9004\n",
      "Epoch 4/50\n",
      "127/127 [==============================] - 2s 19ms/step - loss: 0.0089 - accuracy: 0.9970 - val_loss: 0.4599 - val_accuracy: 0.8994\n",
      "Epoch 5/50\n",
      "127/127 [==============================] - 2s 17ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.5601 - val_accuracy: 0.8876\n",
      "Epoch 6/50\n",
      "127/127 [==============================] - 2s 16ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.6777 - val_accuracy: 0.8757\n",
      "Epoch 7/50\n",
      "127/127 [==============================] - 2s 18ms/step - loss: 4.5255e-05 - accuracy: 1.0000 - val_loss: 0.7619 - val_accuracy: 0.8925\n",
      "Epoch 8/50\n",
      "127/127 [==============================] - 2s 17ms/step - loss: 2.7916e-06 - accuracy: 1.0000 - val_loss: 0.8891 - val_accuracy: 0.8886\n",
      "Epoch 9/50\n",
      "127/127 [==============================] - 2s 15ms/step - loss: 2.6425e-08 - accuracy: 1.0000 - val_loss: 0.9513 - val_accuracy: 0.8935\n",
      "Epoch 10/50\n",
      "127/127 [==============================] - 2s 15ms/step - loss: 6.3891e-09 - accuracy: 1.0000 - val_loss: 0.9853 - val_accuracy: 0.8945\n",
      "Epoch 11/50\n",
      "127/127 [==============================] - 2s 15ms/step - loss: 2.8733e-09 - accuracy: 1.0000 - val_loss: 1.0095 - val_accuracy: 0.8955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea01f655e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_token, y_train, \n",
    "          epochs=50, \n",
    "          batch_size=32,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[es],\n",
    "        use_multiprocessing=True\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c973bb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2481 - accuracy: 0.8958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24811822175979614, 0.8958168625831604]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_token, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceec928",
   "metadata": {},
   "source": [
    "# Attention: Beggining of Word2Vec\n",
    "## Better run only on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f81cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dc26958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
     ]
    }
   ],
   "source": [
    "#word2vec = api.load('glove-wiki-gigaword-50') # this one crashes my kernel everytime\n",
    "word2vec = api.load('text8') # this one takes an eternity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d6de209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d41eea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a79b5da9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Embed the training and test sentences\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train_embed \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword2vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m X_test_embed \u001b[38;5;241m=\u001b[39m embedding(word2vec, X_test)\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(word2vec, sentences)\u001b[0m\n\u001b[1;32m      3\u001b[0m embed \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m----> 6\u001b[0m     embedded_sentence \u001b[38;5;241m=\u001b[39m \u001b[43membed_sentence_with_TF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword2vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     embed\u001b[38;5;241m.\u001b[39mappend(embedded_sentence)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embed\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36membed_sentence_with_TF\u001b[0;34m(word2vec, sentence)\u001b[0m\n\u001b[1;32m      3\u001b[0m embedded_sentence \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence:\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword2vec\u001b[49m:\n\u001b[1;32m      6\u001b[0m         embedded_sentence\u001b[38;5;241m.\u001b[39mappend(word2vec[word])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(embedded_sentence)\n",
      "File \u001b[0;32m~/gensim-data/text8/__init__.py:13\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     12\u001b[0m     corpus \u001b[38;5;241m=\u001b[39m Text8Corpus(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn)\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m corpus:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m doc\n",
      "File \u001b[0;32m~/.pyenv/versions/fake-news/lib/python3.8/site-packages/gensim/models/word2vec.py:2051\u001b[0m, in \u001b[0;36mText8Corpus.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   2050\u001b[0m last_token \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mrfind(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# last token may have been split in two... keep for next iteration\u001b[39;00m\n\u001b[0;32m-> 2051\u001b[0m words, rest \u001b[38;5;241m=\u001b[39m (\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_unicode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mlast_token\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   2052\u001b[0m                text[last_token:]\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mif\u001b[39;00m last_token \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m ([], text)\n\u001b[1;32m   2053\u001b[0m sentence\u001b[38;5;241m.\u001b[39mextend(words)\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sentence) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_sentence_length:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec, X_train)\n",
    "X_test_embed = embedding(word2vec, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e2c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embed = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=300)\n",
    "X_test_embed = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in [X_train_embed, X_test_embed]:\n",
    "    assert type(X) == np.ndarray\n",
    "    assert X.shape[-1] == word2vec.vector_size\n",
    "\n",
    "\n",
    "assert X_train_embed.shape[0] == len(X_train)\n",
    "assert X_test_embed.shape[0] == len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c2553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0, input_shape=X_train_embed.shape[1:]))\n",
    "#model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Conv1D(30, kernel_size=5, activation='tanh'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(15, activation='relu'))\n",
    "#model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='rmsprop',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259bd85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c6d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_embed, y_train, \n",
    "          epochs=50, \n",
    "          batch_size=32,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[es],\n",
    "        use_multiprocessing=True\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26821961",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_embed, y_test, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
